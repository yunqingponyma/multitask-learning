{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fb78a7-702b-48a1-bcaf-1489abc31fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca568a5-c873-4a60-923a-22f953451cce",
   "metadata": {},
   "source": [
    "Let:\n",
    "- $M$ be the total number of features in the feature matrix $X$. These features are indexed from 1 to $M$.\n",
    "- $K$ be the total number of outcomes.\n",
    "- $M_{\\text {shared }}$ be the number of features shared among all $K$ outcomes.\n",
    "- $M_{\\text {specific, } k}$ be the number of features that specifically influence outcome $k$ (for $k=$ $1, \\ldots, K)$, in addition to the shared features.\n",
    "\n",
    "- 1. Shared Feature Set ( $I_{\\text {shared }}$ ):\n",
    "\n",
    "The first $M_{\\text {shared }}$ features of $X$ are designated as shared features. The set of indices for these shared features is:\n",
    "\n",
    "$$\n",
    "I_{\\text {shared }}=\\left\\{f \\mid 1 \\leq f \\leq M_{\\text {shared }}\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "These features influence all $K$ outcomes.  \n",
    "\n",
    "- 2. Specific Feature Sets ( $I_{\\text {specific }, k}$ ):\n",
    "\n",
    "For each outcome $k \\in\\{1, \\ldots, K\\}$, a unique block of $M_{\\text {specific, } k}$ features is allocated to specifically influence that outcome. These blocks are contiguous and follow the shared features and the specific features of any preceding outcomes.\n",
    "\n",
    "Let $S_0=M_{\\text {shared }}$.\n",
    "For $k \\geq 1$, the starting index for the specific features of outcome $k$ is:\n",
    "\n",
    "$$\n",
    "i d x_{s t a r t, k}=S_{k-1}+1\n",
    "$$\n",
    "\n",
    "where $S_{k-1}=M_{\\text {shared }}+\\sum_{l=1}^{k-1} M_{\\text {specific, } l}$ (with the convention that $\\sum_{l=1}^0 M_{\\text {specific, } l}=0$ ).\n",
    "\n",
    "The set of indices for the specific features for outcome $k$ is then:\n",
    "\n",
    "$$\n",
    "I_{\\text {specific }, k}=\\left\\{f \\mid i d x_{\\text {start }, k} \\leq f<i d x_{\\text {start }, k}+M_{\\text {specific }, k}\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "The binary outcome $Y_{j, k}$ for the $j$-th sample and the $k$-th outcome is generated as follows:\n",
    "\n",
    "$$\n",
    "Y_{j, k}=\\mathbb{I}\\left(\\left(\\sum_{f \\in I_{\\text {rclevant, } k}} X_{j, f} \\cdot w_{f, k}\\right)+\\epsilon_j>\\theta_k\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\mathbb{I}(\\cdot)$ : This is the indicator function (it returns 1 if the condition inside is true, and 0 otherwise).\n",
    "- $X_{j, f}$ : This is the value of the $f$-th feature for the $j$-th sample.\n",
    "- $w_{f, k}$ : This is the random weight assigned to feature $f$ for its contribution to outcome $k$.\n",
    "- $I_{\\text {relevant }, k}$ : This is the set of indices for the features that are relevant to (influence) outcome $k$\n",
    "- $\\epsilon_j$ : This represents random noise added to the $j$-th sample.\n",
    "- $\\theta_k$ : This is the threshold calculated based on the desired proportion $P_k$ for outcome $k$ (i.e., the proportion of samples where $Y_{j, k}=1$ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36e8926-6bbe-48f2-997b-58416d6a7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulation_data(n_samples,\n",
    "                             n_total_features,\n",
    "                             n_outcomes,\n",
    "                             n_shared_features,\n",
    "                             n_specific_features_list,\n",
    "                             outcome_proportions):\n",
    "    \"\"\"\n",
    "    生成用于多任务学习模拟的特征矩阵 X 和结果矩阵 Y。\n",
    "\n",
    "    参数:\n",
    "    n_samples (int): 生成的样本数量（行数）。\n",
    "    n_total_features (int): 特征矩阵 X 的总列数。\n",
    "    n_outcomes (int): 要生成的结果数量。\n",
    "    n_shared_features (int): 所有结果共享的特征数量。\n",
    "    n_specific_features_list (list of int): 每个结果额外受其影响的特定特征数量列表。\n",
    "                                       列表长度应与 n_outcomes 相同。\n",
    "    outcome_proportions (list of float): 每个结果中 outcome 为 1 的比例列表。\n",
    "                                     列表长度应与 n_outcomes 相同。\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame: 特征矩阵 X。\n",
    "    pandas.DataFrame: 结果矩阵 Y。\n",
    "    \"\"\"\n",
    "\n",
    "    if len(n_specific_features_list) != n_outcomes:\n",
    "        raise ValueError(\"n_specific_features_list 的长度必须等于 n_outcomes。\")\n",
    "    if len(outcome_proportions) != n_outcomes:\n",
    "        raise ValueError(\"outcome_proportions 的长度必须等于 n_outcomes。\")\n",
    "    if n_shared_features + sum(n_specific_features_list) > n_total_features:\n",
    "        raise ValueError(\"共享特征和所有特定特征的总和不能超过总特征数。\")\n",
    "\n",
    "    # 1. 生成特征矩阵 X\n",
    "    X = np.random.randn(n_samples, n_total_features)\n",
    "    X_df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_total_features)])\n",
    "\n",
    "    # 2. 生成结果矩阵 Y\n",
    "    Y_data = np.zeros((n_samples, n_outcomes))\n",
    "\n",
    "    # 定义特征索引\n",
    "    shared_feature_indices = list(range(n_shared_features))\n",
    "    current_specific_start_idx = n_shared_features\n",
    "\n",
    "    for i in range(n_outcomes):\n",
    "        # 确定影响当前结果的特征列\n",
    "        specific_feature_indices = list(range(current_specific_start_idx,\n",
    "                                            current_specific_start_idx + n_specific_features_list[i]))\n",
    "        relevant_feature_indices = shared_feature_indices + specific_feature_indices\n",
    "\n",
    "        # 为这些特征生成随机权重 (也可以根据需要设置固定的权重)\n",
    "        # 权重的数量应该等于 relevant_feature_indices 的长度\n",
    "        weights = np.random.randn(len(relevant_feature_indices))\n",
    "\n",
    "        # 计算线性组合\n",
    "        linear_combination = X[:, relevant_feature_indices] @ weights\n",
    "\n",
    "        # 添加一些随机噪声，使得结果不完全由特征决定\n",
    "        noise = np.random.randn(n_samples) * 1.0 # 噪声的标准差可以调整\n",
    "        signal_with_noise = linear_combination + noise\n",
    "\n",
    "        # 根据指定的比例确定阈值，以生成二元结果\n",
    "        # 这里我们使用百分位数作为阈值\n",
    "        # outcome_proportions[i] 意味着 (1 - outcome_proportions[i]) 的百分位数是阈值\n",
    "        # 例如，如果比例是 3%，那么高于第 97 个百分位的值将被设为 1\n",
    "        threshold = np.percentile(signal_with_noise, 100 * (1 - outcome_proportions[i]))\n",
    "        \n",
    "        Y_data[:, i] = (signal_with_noise > threshold).astype(int)\n",
    "\n",
    "        # 更新下一个特定特征的起始索引\n",
    "        current_specific_start_idx += n_specific_features_list[i]\n",
    "\n",
    "    Y_df = pd.DataFrame(Y_data, columns=[f'outcome_{j+1}' for j in range(n_outcomes)], dtype=int)\n",
    "\n",
    "    return X_df, Y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e41bc-a561-49ee-a52f-9c12eeb973fa",
   "metadata": {},
   "source": [
    "Let:\n",
    "- $X_j$ be the feature vector for the $j$-th sample.\n",
    "- $X_{j, I_{\\text {shared }}}$ be the sub-vector of $X_j$ containing only the shared features.\n",
    "- $X_{j, I_{\\text {specific, } k}}$ be the sub-vector of $X_j$ containing only the specific features for outcome $k$.\n",
    "- $w_{\\text {shared }}$ be a vector of ones with dimension $M_{\\text {shared }}$ (since weights_shared = np.ones(len(shared_feature_indices))).\n",
    "- $w_{\\text {specific, } k}$ be a vector of ones with dimension $M_{\\text {specific, } k}$ (since weights_specific = np.ones(len(specific_feature_indices))).\n",
    "\n",
    "1. Shared Component ( $C_{\\text {shared }, j}$ ):\n",
    "\n",
    "\n",
    "\n",
    "Let $Z_{\\text {shared, } j}=X_{j, I_{\\text {shared }}} \\cdot w_{\\text {shared }}=\\sum_{f \\in I_{\\text {shared }}} X_{j, f}$ (since $w_{\\text {shared }}$ contains ones).  \n",
    "Then, the shared component for sample $j$ can be written as:\n",
    "\n",
    "$$\n",
    "C_{\\text {shared }, j}=\\tanh \\left(0.5 \\cdot Z_{\\text {shared }, j}\\right)+\\cos \\left(0.5 \\cdot Z_{\\text {shared }, j}\\right)+\\left(0.3 \\cdot Z_{\\text {shared }, j}\\right)^2\n",
    "$$\n",
    "\n",
    "2. Specific Component $\\left(C_{\\text {specific }, j, k}\\right)$ :\n",
    "\n",
    "\n",
    "Let $Z_{\\text {specific }, j, k}=X_{j, I_{\\text {specific }, k}} \\cdot w_{\\text {specific }, k}=\\sum_{f \\in I_{\\text {specific, } k}} X_{j, f}$ (since $w_{\\text {specific }, k}$ contains ones).  \n",
    "\n",
    "Then, the specific component for sample $j$ and outcome $k$ is:\n",
    "\n",
    "$$\n",
    "C_{\\text {specific }, j, k}=Z_{\\text {specific }, j, k}\n",
    "$$\n",
    "\n",
    "3. Combined Pre-Noise Signal ( $L_{j, k}$ ):\n",
    "\n",
    "The linear_combination in the code depends on the outcome index i (which corresponds to $k-1$ if $k$ is 1 -indexed for outcomes). Let $\\alpha_k$ and $\\beta_k$ be the weighting factors for the shared and specific components for outcome $k$.  \n",
    "\n",
    "- For outcome $k=1$ (i=0): $\\alpha_1=1.5, \\beta_1=0.5$\n",
    "- For outcome $k=2$ ( $\\mathrm{i}=1$ ): $\\alpha_2=1.0, \\beta_2=1.0$\n",
    "- For outcome $k=3$ ( $\\mathrm{i}=2$ ): $\\alpha_3=0.5, \\beta_3=1.5$\n",
    "\n",
    "So, the combined signal before noise for sample $j$ and outcome $k$ is:\n",
    "\n",
    "$$\n",
    "L_{j, k}=\\alpha_k \\cdot C_{\\text {shared }, j}+\\beta_k \\cdot C_{\\text {specific }, j, k}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "L_{j, k}=\\alpha_k\\left(\\tanh \\left(0.5 Z_{\\text {shared }, j}\\right)+\\cos \\left(0.5 Z_{\\text {shared }, j}\\right)+\\left(0.3 Z_{\\text {shared }, j}\\right)^2\\right)+\\beta_k Z_{\\text {specific }, j, k}\n",
    "$$\n",
    "\n",
    "4. Binary Outcome Generation ( $Y_{j, k}$ ):\n",
    "\n",
    "The binary outcome $Y_{j, k}$ for the $j$-th sample and the $k$-th outcome is generated by adding noise and then thresholding:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "S_{j, k}=L_{j, k}+\\epsilon_j \\\\\n",
    "Y_{j, k}=\\mathbb{I}\\left(S_{j, k}>\\theta_k\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbb{I}(\\cdot)$ : This is the indicator function (it returns 1 if the condition inside is true, and 0 otherwise).\n",
    "- $\\epsilon_j$ : This represents random noise added to the $j$-th sample, $\\epsilon_j \\sim \\mathcal{N}\\left(0, \\sigma_{\\text {noise }}^2\\right)$ (in your code, $\\sigma_{\\text {noise }}=1.5$ ).\n",
    "- $\\theta_k$ : This is the threshold calculated based on the desired proportion $P_k$ for outcome $k$ (i.e., the proportion of samples where $Y_{j, k}=1$ ). It's the $100 \\times\\left(1-P_k\\right)$-th percentile of the distribution of $S_{j, k}$ over all samples for outcome $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a7ceef-d107-4a1a-affd-ab858da5218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulation_data(n_samples,\n",
    "                             n_total_features,\n",
    "                             n_outcomes,\n",
    "                             n_shared_features,\n",
    "                             n_specific_features_list,\n",
    "                             outcome_proportions):\n",
    "    \"\"\"\n",
    "    生成用于多任务学习模拟的特征矩阵 X 和结果矩阵 Y。\n",
    "\n",
    "    参数:\n",
    "    n_samples (int): 生成的样本数量（行数）。\n",
    "    n_total_features (int): 特征矩阵 X 的总列数。\n",
    "    n_outcomes (int): 要生成的结果数量。\n",
    "    n_shared_features (int): 所有结果共享的特征数量。\n",
    "    n_specific_features_list (list of int): 每个结果额外受其影响的特定特征数量列表。\n",
    "                                       列表长度应与 n_outcomes 相同。\n",
    "    outcome_proportions (list of float): 每个结果中 outcome 为 1 的比例列表。\n",
    "                                     列表长度应与 n_outcomes 相同。\n",
    "\n",
    "    返回:\n",
    "    pandas.DataFrame: 特征矩阵 X。\n",
    "    pandas.DataFrame: 结果矩阵 Y。\n",
    "    \"\"\"\n",
    "\n",
    "    if len(n_specific_features_list) != n_outcomes:\n",
    "        raise ValueError(\"n_specific_features_list 的长度必须等于 n_outcomes。\")\n",
    "    if len(outcome_proportions) != n_outcomes:\n",
    "        raise ValueError(\"outcome_proportions 的长度必须等于 n_outcomes。\")\n",
    "    if n_shared_features + sum(n_specific_features_list) > n_total_features:\n",
    "        raise ValueError(\"共享特征和所有特定特征的总和不能超过总特征数。\")\n",
    "\n",
    "    # 1. 生成特征矩阵 X\n",
    "    X = np.random.randn(n_samples, n_total_features)\n",
    "    X_df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_total_features)])\n",
    "\n",
    "    # 2. 生成结果矩阵 Y\n",
    "    Y_data = np.zeros((n_samples, n_outcomes))\n",
    "\n",
    "    # 定义特征索引\n",
    "    shared_feature_indices = list(range(n_shared_features))\n",
    "    current_specific_start_idx = n_shared_features\n",
    "\n",
    "    for i in range(n_outcomes):\n",
    "        specific_feature_indices = list(range(current_specific_start_idx,\n",
    "                                            current_specific_start_idx + n_specific_features_list[i]))\n",
    "        relevant_feature_indices = shared_feature_indices + specific_feature_indices\n",
    "        \n",
    "        # weights_shared = np.random.randn(len(shared_feature_indices))\n",
    "        # weights_specific = np.random.randn(len(specific_feature_indices))\n",
    "\n",
    "        weights_shared = np.ones(len(shared_feature_indices))\n",
    "        weights_specific = np.ones(len(specific_feature_indices))\n",
    "\n",
    "        # Introduce some non-linearity for shared features\n",
    "        # This non-linear component is common to all tasks, but its impact is weighted\n",
    "        shared_component_nonlinear = np.tanh(X[:, shared_feature_indices] @ (weights_shared * 0.5)) + \\\n",
    "                                     np.cos(X[:, shared_feature_indices] @ (weights_shared * 0.5)) + \\\n",
    "                                     np.square(X[:, shared_feature_indices] @ (weights_shared * 0.3))     \n",
    "\n",
    "        # Linear component from specific features\n",
    "        specific_component_linear = X[:, specific_feature_indices] @ weights_specific\n",
    "\n",
    "        # Combine components - maybe shared part is more dominant or interacts\n",
    "        # Let's say for outcome 1, shared is more important, for outcome 3, specific is.\n",
    "        if i == 0: # Outcome 1\n",
    "            linear_combination = shared_component_nonlinear * 1.5 + specific_component_linear * 0.5\n",
    "        elif i == 1: # Outcome 2\n",
    "            linear_combination = shared_component_nonlinear * 1.0 + specific_component_linear * 1.0\n",
    "        else: # Outcome 3\n",
    "            linear_combination = shared_component_nonlinear * 0.5 + specific_component_linear * 1.5\n",
    "\n",
    "        noise = np.random.randn(n_samples) * 1.5 # Increased noise\n",
    "        signal_with_noise = linear_combination + noise\n",
    "        \n",
    "        threshold = np.percentile(signal_with_noise, 100 * (1 - outcome_proportions[i]))\n",
    "        Y_data[:, i] = (signal_with_noise > threshold).astype(int)\n",
    "        \n",
    "        current_specific_start_idx += n_specific_features_list[i]\n",
    "\n",
    "    Y_df = pd.DataFrame(Y_data, columns=[f'outcome_{j+1}' for j in range(n_outcomes)], dtype=int)\n",
    "\n",
    "    return X_df, Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a741508-96b3-49cc-b141-07cbfe7563e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征矩阵 X (前5行):\n",
      "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0   1.367207  -0.212154   0.945395   0.155082   0.897312   1.558312   \n",
      "1  -1.350954   0.265787   1.766339   0.256847   0.283624  -1.040460   \n",
      "2   0.585420   0.298662  -0.013306  -0.107048   0.427913   1.463934   \n",
      "3   0.822995  -1.370238  -0.101783  -1.538012  -0.592612  -0.436027   \n",
      "4  -2.147712   0.039841  -0.464098   0.099351  -0.933959   0.187428   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  ...  feature_41  feature_42  \\\n",
      "0   1.666489   0.505759   0.188817    0.836058  ...   -1.184929   -0.181021   \n",
      "1  -0.957071   1.314946   1.121816   -0.665375  ...    0.525489   -0.281244   \n",
      "2   0.140767   0.551922  -1.207128    0.240290  ...   -1.024939   -0.536162   \n",
      "3  -0.872720  -0.108093   0.354522    0.040711  ...   -0.142007    0.673951   \n",
      "4  -0.197051  -0.992226   0.204837    0.482590  ...    1.613960    0.721262   \n",
      "\n",
      "   feature_43  feature_44  feature_45  feature_46  feature_47  feature_48  \\\n",
      "0   -0.207103    1.939751    0.787410    0.324977    1.429742    0.454312   \n",
      "1    0.976924    0.141937    2.137482    1.760394    0.327058    0.182077   \n",
      "2   -0.034122   -1.508269    0.074466   -0.300320   -1.878899    0.000506   \n",
      "3   -1.290606   -0.631414   -0.667729   -0.688916    1.217419   -0.274754   \n",
      "4    0.415980    0.501638    1.337047   -0.245897   -0.117325    0.227153   \n",
      "\n",
      "   feature_49  feature_50  \n",
      "0    1.038304   -1.725499  \n",
      "1   -0.912800   -0.827857  \n",
      "2    0.793087    1.450687  \n",
      "3   -0.356152    0.551006  \n",
      "4   -0.219750    1.623700  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "结果矩阵 Y (前5行):\n",
      "   outcome_1  outcome_2  outcome_3\n",
      "0          0          0          1\n",
      "1          0          0          1\n",
      "2          0          0          0\n",
      "3          0          0          0\n",
      "4          0          0          1\n",
      "\n",
      "X的形状: (500, 50)\n",
      "Y的形状: (500, 3)\n",
      "\n",
      "每个Outcome中1的实际比例:\n",
      "Outcome 1: 0.0300 (期望: 0.03)\n",
      "Outcome 2: 0.0500 (期望: 0.05)\n",
      "Outcome 3: 0.1200 (期望: 0.12)\n",
      "\n",
      "共享特征列名: ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20']\n",
      "Outcome 1 的特定特征列名: ['feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30']\n",
      "Outcome 2 的特定特征列名: ['feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40']\n",
      "Outcome 3 的特定特征列名: ['feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50']\n"
     ]
    }
   ],
   "source": [
    "# --- 参数设置 ---\n",
    "n_rows = 500  # 样本数量\n",
    "n_total_cols_X = 50 # X的总列数\n",
    "num_outcomes = 3   # 结果的数量\n",
    "\n",
    "# 特征分配\n",
    "shared_cols = 20  # 共享特征的数量\n",
    "# 每个outcome额外单独受影响的特征数量\n",
    "# outcome 1 单独受 10 列影响\n",
    "# outcome 2 单独受 10 列影响\n",
    "# outcome 3 单独受 10 列影响\n",
    "# 20 (共享) + 10 (O1) + 10 (O2) + 10 (O3) = 50 (总特征数)\n",
    "specific_cols_per_outcome = [10, 10, 10]\n",
    "\n",
    "# 结果为1的比例\n",
    "proportions_of_ones = [0.03, 0.05, 0.12] # outcome1=3%, outcome2=5%, outcome3=10%\n",
    "\n",
    "# --- 生成数据 ---\n",
    "X_sim, Y_sim = generate_simulation_data(\n",
    "    n_samples=n_rows,\n",
    "    n_total_features=n_total_cols_X,\n",
    "    n_outcomes=num_outcomes,\n",
    "    n_shared_features=shared_cols,\n",
    "    n_specific_features_list=specific_cols_per_outcome,\n",
    "    outcome_proportions=proportions_of_ones\n",
    ")\n",
    "\n",
    "# --- 打印一些信息和数据头部 ---\n",
    "print(\"特征矩阵 X (前5行):\")\n",
    "print(X_sim.head())\n",
    "print(\"\\n结果矩阵 Y (前5行):\")\n",
    "print(Y_sim.head())\n",
    "\n",
    "print(\"\\nX的形状:\", X_sim.shape)\n",
    "print(\"Y的形状:\", Y_sim.shape)\n",
    "\n",
    "print(\"\\n每个Outcome中1的实际比例:\")\n",
    "for i in range(num_outcomes):\n",
    "    actual_proportion = Y_sim[f'outcome_{i+1}'].mean()\n",
    "    print(f\"Outcome {i+1}: {actual_proportion:.4f} (期望: {proportions_of_ones[i]})\")\n",
    "\n",
    "print(\"\\n共享特征列名:\", X_sim.columns[:shared_cols].tolist())\n",
    "current_idx = shared_cols\n",
    "for i in range(num_outcomes):\n",
    "    print(f\"Outcome {i+1} 的特定特征列名:\", X_sim.columns[current_idx : current_idx + specific_cols_per_outcome[i]].tolist())\n",
    "    current_idx += specific_cols_per_outcome[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb1dd4-e908-4485-aa2f-119a05bff352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c959e8bd-93b7-44b5-8485-4b8d8e8fe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred_binary):\n",
    "    \"\"\"计算准确率\"\"\"\n",
    "    return accuracy_score(y_true, y_pred_binary)\n",
    "\n",
    "def calculate_auc(y_true, y_pred_proba):\n",
    "    \"\"\"计算AUC\"\"\"\n",
    "    # 确保 y_pred_proba 是正类的概率\n",
    "    if y_pred_proba.ndim == 2 and y_pred_proba.shape[1] == 2:\n",
    "        y_pred_proba = y_pred_proba[:, 1]\n",
    "    return roc_auc_score(y_true, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd5e273-f218-4e66-8436-2ef3da306295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 划分训练集和测试集 ---\n",
    "# 我们将对整个X和Y进行一次划分，然后针对每个outcome进行训练\n",
    "# stratify=Y_sim 可以在多标签情况下帮助保持类别比例，但对于非常稀疏的多标签组合可能不完美\n",
    "# 如果Y_sim中有很多全零或全一的行，或者标签组合非常多，stratify可能效果不佳或报错\n",
    "# 在这种情况下，可以考虑不使用stratify，或者对每个outcome单独划分（但这样X_train/X_test会不同）\n",
    "# 这里我们尝试对Y进行整体分层，如果报错，可以改成 stratify=None\n",
    "try:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_sim, Y_sim, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"分层抽样失败: {e}. 将尝试不使用分层。\")\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_sim, Y_sim, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d148929-e8b8-48da-8ff5-3f5d88741f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ce18bf-20f2-4f81-8bc5-39798acefd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 训练和评估 Outcome: outcome_1 ---\n",
      "模型截距 (Intercept) for outcome_1: -4.8124\n",
      "测试集 Accuracy for outcome_1: 0.9100\n",
      "测试集 AUC for outcome_1: 0.6186\n",
      "\n",
      "--- 训练和评估 Outcome: outcome_2 ---\n",
      "模型截距 (Intercept) for outcome_2: -4.5774\n",
      "测试集 Accuracy for outcome_2: 0.8900\n",
      "测试集 AUC for outcome_2: 0.6000\n",
      "\n",
      "--- 训练和评估 Outcome: outcome_3 ---\n",
      "模型截距 (Intercept) for outcome_3: -3.3899\n",
      "测试集 Accuracy for outcome_3: 0.9100\n",
      "测试集 AUC for outcome_3: 0.9357\n",
      "\n",
      "\n",
      "--- 单任务基线模型结果总结 ---\n",
      "\n",
      "Outcome: outcome_1\n",
      "  Accuracy: 0.9100\n",
      "  AUC: 0.6186\n",
      "\n",
      "Outcome: outcome_2\n",
      "  Accuracy: 0.8900\n",
      "  AUC: 0.6000\n",
      "\n",
      "Outcome: outcome_3\n",
      "  Accuracy: 0.9100\n",
      "  AUC: 0.9357\n"
     ]
    }
   ],
   "source": [
    "baseline_results = {}\n",
    "\n",
    "for i in range(num_outcomes):\n",
    "    outcome_col_name = f'outcome_{i+1}'\n",
    "    print(f\"\\n--- 训练和评估 Outcome: {outcome_col_name} ---\")\n",
    "\n",
    "    y_train_outcome = Y_train[outcome_col_name]\n",
    "    y_test_outcome = Y_test[outcome_col_name]\n",
    "\n",
    "    # 检查训练集中是否有足够的多样性\n",
    "    if len(np.unique(y_train_outcome)) < 2:\n",
    "        print(f\"Outcome {outcome_col_name} 在训练集中只有一个类别，跳过此outcome。\")\n",
    "        continue\n",
    "\n",
    "    # --- 2. 初始化和训练逻辑回归模型 ---\n",
    "    # L2正则化默认开启，C是正则化强度的倒数，较小的C表示更强的正则化\n",
    "    # solver='liblinear' 适用于小型数据集和二分类问题\n",
    "    # class_weight='balanced' 对于类别不平衡的数据集有帮助\n",
    "    model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42, C=1.0)\n",
    "    model.fit(X_train, y_train_outcome)\n",
    "\n",
    "    # --- 3. 获取beta系数 ---\n",
    "    # model.coef_ 是一个二维数组 (1, n_features)\n",
    "    betas = model.coef_[0]\n",
    "    intercept = model.intercept_[0]\n",
    "    print(f\"模型截距 (Intercept) for {outcome_col_name}: {intercept:.4f}\")\n",
    "    # print(f\"模型系数 (Betas) for {outcome_col_name}:\\n{betas}\") # 如果特征很多，打印会很长\n",
    "\n",
    "    # --- 4. 在测试集上进行预测 ---\n",
    "    y_pred_proba_outcome = model.predict_proba(X_test) # 获取概率\n",
    "    y_pred_binary_outcome = model.predict(X_test)      # 获取二元预测 (0或1)\n",
    "\n",
    "    # --- 5. 计算并记录 Accuracy 和 AUC ---\n",
    "    accuracy = calculate_accuracy(y_test_outcome, y_pred_binary_outcome)\n",
    "    # predict_proba 返回两列，第二列是正类（1）的概率\n",
    "    auc = calculate_auc(y_test_outcome, y_pred_proba_outcome[:, 1])\n",
    "\n",
    "    baseline_results[outcome_col_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'betas': betas,\n",
    "        'intercept': intercept\n",
    "    }\n",
    "\n",
    "    print(f\"测试集 Accuracy for {outcome_col_name}: {accuracy:.4f}\")\n",
    "    print(f\"测试集 AUC for {outcome_col_name}: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n\\n--- 单任务基线模型结果总结 ---\")\n",
    "for outcome, metrics in baseline_results.items():\n",
    "    print(f\"\\nOutcome: {outcome}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ebb27-5d59-46ba-b938-8e6e038cbe1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a864b3-19b0-4aa7-a8d6-6e40d56188c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 训练和评估 Outcome: outcome_1 使用Lasso (C=0.1) ---\n",
      "Lasso模型截距 for outcome_1: -1.2730\n",
      "Lasso模型中接近零的Beta系数数量 for outcome_1: 22 / 50\n",
      "测试集 Lasso Accuracy for outcome_1: 0.7800\n",
      "测试集 Lasso AUC for outcome_1: 0.6151\n",
      "\n",
      "--- 训练和评估 Outcome: outcome_2 使用Lasso (C=0.1) ---\n",
      "Lasso模型截距 for outcome_2: -1.3561\n",
      "Lasso模型中接近零的Beta系数数量 for outcome_2: 26 / 50\n",
      "测试集 Lasso Accuracy for outcome_2: 0.8700\n",
      "测试集 Lasso AUC for outcome_2: 0.6337\n",
      "\n",
      "--- 训练和评估 Outcome: outcome_3 使用Lasso (C=0.1) ---\n",
      "Lasso模型截距 for outcome_3: -0.8318\n",
      "Lasso模型中接近零的Beta系数数量 for outcome_3: 27 / 50\n",
      "测试集 Lasso Accuracy for outcome_3: 0.9100\n",
      "测试集 Lasso AUC for outcome_3: 0.9851\n",
      "\n",
      "\n",
      "--- Lasso (L1正则化逻辑回归) 单任务模型结果总结 ---\n",
      "\n",
      "Outcome: outcome_1\n",
      "  Accuracy: 0.7800\n",
      "  AUC: 0.6151\n",
      "  接近零的Beta系数数量: 22 / 50\n",
      "  非零系数的特征:\n",
      "    feature_1: 0.5435\n",
      "    feature_3: 0.1188\n",
      "    feature_4: 0.0030\n",
      "    feature_6: 0.3325\n",
      "    feature_7: -0.0875\n",
      "    feature_8: 0.3353\n",
      "    feature_9: 0.2576\n",
      "    feature_11: 0.3174\n",
      "    feature_12: 0.1980\n",
      "    feature_17: 0.0539\n",
      "    feature_20: 0.5158\n",
      "    feature_24: 0.0292\n",
      "    feature_27: -0.0106\n",
      "    feature_28: -0.4372\n",
      "    feature_29: 0.1701\n",
      "    feature_30: 0.2065\n",
      "    feature_31: 0.0205\n",
      "    feature_32: 0.0046\n",
      "    feature_33: -0.1914\n",
      "    feature_34: -0.0287\n",
      "    feature_35: -0.3561\n",
      "    feature_36: 0.1697\n",
      "    feature_44: -0.0272\n",
      "    feature_45: -0.0755\n",
      "    feature_47: 0.0142\n",
      "    feature_48: -0.0530\n",
      "    feature_49: -0.0889\n",
      "    feature_50: -0.4444\n",
      "\n",
      "Outcome: outcome_2\n",
      "  Accuracy: 0.8700\n",
      "  AUC: 0.6337\n",
      "  接近零的Beta系数数量: 26 / 50\n",
      "  非零系数的特征:\n",
      "    feature_1: 0.5842\n",
      "    feature_3: 0.0394\n",
      "    feature_4: 0.1008\n",
      "    feature_5: 0.1520\n",
      "    feature_6: 0.1178\n",
      "    feature_8: 0.1345\n",
      "    feature_9: 0.5427\n",
      "    feature_11: 0.1514\n",
      "    feature_12: 0.6311\n",
      "    feature_13: 0.3392\n",
      "    feature_14: 0.0499\n",
      "    feature_15: 0.0262\n",
      "    feature_16: 0.1182\n",
      "    feature_17: 0.0685\n",
      "    feature_22: 0.4390\n",
      "    feature_26: 0.4151\n",
      "    feature_28: -0.4272\n",
      "    feature_31: 0.0111\n",
      "    feature_32: 0.2758\n",
      "    feature_33: 0.0013\n",
      "    feature_36: 0.3520\n",
      "    feature_39: 0.2237\n",
      "    feature_40: 0.2592\n",
      "    feature_50: -0.0086\n",
      "\n",
      "Outcome: outcome_3\n",
      "  Accuracy: 0.9100\n",
      "  AUC: 0.9851\n",
      "  接近零的Beta系数数量: 27 / 50\n",
      "  非零系数的特征:\n",
      "    feature_4: 0.1996\n",
      "    feature_5: 0.2260\n",
      "    feature_7: 0.1436\n",
      "    feature_9: 0.0898\n",
      "    feature_11: 0.0500\n",
      "    feature_13: 0.0367\n",
      "    feature_14: -0.0020\n",
      "    feature_15: 0.0940\n",
      "    feature_20: 0.0433\n",
      "    feature_22: 0.1653\n",
      "    feature_28: 0.0115\n",
      "    feature_37: -0.2182\n",
      "    feature_40: -0.0901\n",
      "    feature_41: 0.4187\n",
      "    feature_42: 0.6698\n",
      "    feature_43: 0.2673\n",
      "    feature_44: 0.2577\n",
      "    feature_45: 0.4570\n",
      "    feature_46: 0.3356\n",
      "    feature_47: 0.3374\n",
      "    feature_48: 0.6603\n",
      "    feature_49: 0.0701\n",
      "    feature_50: 0.3719\n"
     ]
    }
   ],
   "source": [
    "lasso_results = {}\n",
    "# C值是正则化强度的倒数。较小的值表示更强的正则化。\n",
    "# 您可以尝试不同的C值来观察其对特征选择和模型性能的影响。\n",
    "C_value_lasso = 0.1 # 这是一个可以调整的超参数\n",
    "\n",
    "for i in range(num_outcomes):\n",
    "    outcome_col_name = f'outcome_{i+1}'\n",
    "    print(f\"\\n--- 训练和评估 Outcome: {outcome_col_name} 使用Lasso (C={C_value_lasso}) ---\")\n",
    "\n",
    "    y_train_outcome = Y_train[outcome_col_name]\n",
    "    y_test_outcome = Y_test[outcome_col_name]\n",
    "\n",
    "    if len(np.unique(y_train_outcome)) < 2:\n",
    "        print(f\"Outcome {outcome_col_name} 在训练集中只有一个类别，跳过此outcome。\")\n",
    "        continue\n",
    "\n",
    "    # --- 初始化和训练带L1正则化的逻辑回归模型 ---\n",
    "    # penalty='l1' 指定使用L1正则化 (Lasso)\n",
    "    # solver='liblinear' 或 'saga' 支持L1正则化。'liblinear' 通常对小型数据集表现良好。\n",
    "    # 对于 'saga'，可能需要增加 max_iter。\n",
    "    model_lasso = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear', # 或者 'saga'\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        C=C_value_lasso,\n",
    "        # max_iter=1000 # 如果使用 'saga' 且不收敛，可以增加此参数\n",
    "    )\n",
    "    model_lasso.fit(X_train, y_train_outcome)\n",
    "\n",
    "    # --- 获取beta系数 ---\n",
    "    betas_lasso = model_lasso.coef_[0]\n",
    "    intercept_lasso = model_lasso.intercept_[0]\n",
    "    print(f\"Lasso模型截距 for {outcome_col_name}: {intercept_lasso:.4f}\")\n",
    "    # print(f\"Lasso模型系数 for {outcome_col_name}:\\n{betas_lasso}\")\n",
    "\n",
    "    num_zero_betas = np.sum(np.abs(betas_lasso) < 1e-6) # 计算近似为零的系数数量\n",
    "    print(f\"Lasso模型中接近零的Beta系数数量 for {outcome_col_name}: {num_zero_betas} / {len(betas_lasso)}\")\n",
    "\n",
    "\n",
    "    # --- 在测试集上进行预测 ---\n",
    "    y_pred_proba_lasso = model_lasso.predict_proba(X_test)[:, 1] # 正类的概率\n",
    "    y_pred_binary_lasso = model_lasso.predict(X_test)\n",
    "\n",
    "    # --- 计算并记录 Accuracy 和 AUC ---\n",
    "    accuracy_lasso = calculate_accuracy(y_test_outcome, y_pred_binary_lasso)\n",
    "    auc_lasso = calculate_auc(y_test_outcome, y_pred_proba_lasso)\n",
    "\n",
    "    lasso_results[outcome_col_name] = {\n",
    "        'accuracy': accuracy_lasso,\n",
    "        'auc': auc_lasso,\n",
    "        'betas': betas_lasso,\n",
    "        'intercept': intercept_lasso,\n",
    "        'num_zero_betas': num_zero_betas\n",
    "    }\n",
    "\n",
    "    print(f\"测试集 Lasso Accuracy for {outcome_col_name}: {accuracy_lasso:.4f}\")\n",
    "    print(f\"测试集 Lasso AUC for {outcome_col_name}: {auc_lasso:.4f}\")\n",
    "\n",
    "    # 可选：绘制ROC曲线 (与之前代码类似)\n",
    "    # fpr_lasso, tpr_lasso, _ = roc_curve(y_test_outcome, y_pred_proba_lasso)\n",
    "    # plt.figure()\n",
    "    # plt.plot(fpr_lasso, tpr_lasso, label=f'{outcome_col_name} Lasso (area = {auc_lasso:.2f})')\n",
    "    # plt.plot([0, 1], [0, 1],'r--')\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.title(f'Lasso ROC Curve for {outcome_col_name}')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    # plt.show()\n",
    "\n",
    "print(\"\\n\\n--- Lasso (L1正则化逻辑回归) 单任务模型结果总结 ---\")\n",
    "for outcome, metrics in lasso_results.items():\n",
    "    print(f\"\\nOutcome: {outcome}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {metrics['auc']:.4f}\")\n",
    "    print(f\"  接近零的Beta系数数量: {metrics['num_zero_betas']} / {n_total_cols_X}\")\n",
    "    # 打印非零系数对应的特征名\n",
    "    print(f\"  非零系数的特征:\")\n",
    "    non_zero_indices = np.where(np.abs(metrics['betas']) > 1e-6)[0]\n",
    "    if len(non_zero_indices) > 0:\n",
    "        for idx in non_zero_indices:\n",
    "            print(f\"    {X_sim.columns[idx]}: {metrics['betas'][idx]:.4f}\")\n",
    "    else:\n",
    "        print(\"    所有特征系数均接近零。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd6f25-8efc-49b7-8e88-cd30fc4aaa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df97a977-237b-4d44-9e40-caa1c7145e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 训练和评估 Outcome: outcome_1 使用DNN ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                3264      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5377 (21.00 KB)\n",
      "Trainable params: 5377 (21.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "类别权重 for outcome_1: {0: 0.5154639175257731, 1: 16.666666666666664}\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 20ms/step - loss: 0.8181 - accuracy: 0.7625 - auc: 0.3882 - val_loss: 0.4925 - val_accuracy: 0.8375 - val_auc: 0.4481\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.8188 - auc: 0.6163 - val_loss: 0.4934 - val_accuracy: 0.8375 - val_auc: 0.4502\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6685 - accuracy: 0.8156 - auc: 0.6033 - val_loss: 0.5015 - val_accuracy: 0.8375 - val_auc: 0.4784\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6319 - accuracy: 0.8250 - auc: 0.6720 - val_loss: 0.5183 - val_accuracy: 0.8375 - val_auc: 0.5152\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5979 - accuracy: 0.8281 - auc: 0.7337 - val_loss: 0.5296 - val_accuracy: 0.8000 - val_auc: 0.5714\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5898 - accuracy: 0.7844 - auc: 0.7092 - val_loss: 0.5339 - val_accuracy: 0.7875 - val_auc: 0.6147\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4725 - accuracy: 0.8406 - auc: 0.9116 - val_loss: 0.5258 - val_accuracy: 0.8000 - val_auc: 0.6104\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4712 - accuracy: 0.8406 - auc: 0.9091 - val_loss: 0.5026 - val_accuracy: 0.8250 - val_auc: 0.6234\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5685 - accuracy: 0.8219 - auc: 0.7749 - val_loss: 0.4986 - val_accuracy: 0.8250 - val_auc: 0.6320\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5853 - accuracy: 0.8344 - auc: 0.7646 - val_loss: 0.4851 - val_accuracy: 0.8250 - val_auc: 0.6364\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.8687 - auc: 0.8867 - val_loss: 0.4717 - val_accuracy: 0.8375 - val_auc: 0.6472\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.8656 - auc: 0.9436 - val_loss: 0.4596 - val_accuracy: 0.8375 - val_auc: 0.6840\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4168 - accuracy: 0.8438 - auc: 0.9507 - val_loss: 0.4416 - val_accuracy: 0.8750 - val_auc: 0.6991\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4713 - accuracy: 0.8562 - auc: 0.8941 - val_loss: 0.4205 - val_accuracy: 0.8750 - val_auc: 0.7251\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4093 - accuracy: 0.8719 - auc: 0.9193 - val_loss: 0.4291 - val_accuracy: 0.8625 - val_auc: 0.7403\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3817 - accuracy: 0.8781 - auc: 0.9437 - val_loss: 0.4202 - val_accuracy: 0.8500 - val_auc: 0.7511\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3009 - accuracy: 0.8938 - auc: 0.9864 - val_loss: 0.3896 - val_accuracy: 0.8625 - val_auc: 0.7511\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.9031 - auc: 0.9055 - val_loss: 0.3704 - val_accuracy: 0.8750 - val_auc: 0.7554\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3029 - accuracy: 0.8813 - auc: 0.9784 - val_loss: 0.3664 - val_accuracy: 0.8625 - val_auc: 0.7597\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2756 - accuracy: 0.8719 - auc: 0.9879 - val_loss: 0.3374 - val_accuracy: 0.8750 - val_auc: 0.7792\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2660 - accuracy: 0.9156 - auc: 0.9889 - val_loss: 0.3162 - val_accuracy: 0.9000 - val_auc: 0.7879\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2277 - accuracy: 0.9187 - auc: 0.9950 - val_loss: 0.2919 - val_accuracy: 0.9000 - val_auc: 0.7879\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2533 - accuracy: 0.9187 - auc: 0.9896 - val_loss: 0.2682 - val_accuracy: 0.9125 - val_auc: 0.7944\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9563 - auc: 0.9950 - val_loss: 0.2526 - val_accuracy: 0.9250 - val_auc: 0.8030\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2199 - accuracy: 0.9438 - auc: 0.9907 - val_loss: 0.2475 - val_accuracy: 0.9125 - val_auc: 0.8095\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2326 - accuracy: 0.9438 - auc: 0.9909 - val_loss: 0.2422 - val_accuracy: 0.9125 - val_auc: 0.8095\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9563 - auc: 0.9937 - val_loss: 0.2374 - val_accuracy: 0.9125 - val_auc: 0.8095\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2357 - accuracy: 0.9469 - auc: 0.9778 - val_loss: 0.2375 - val_accuracy: 0.9125 - val_auc: 0.8095\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9531 - auc: 0.9971 - val_loss: 0.2251 - val_accuracy: 0.9125 - val_auc: 0.7965\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9531 - auc: 0.9993 - val_loss: 0.2108 - val_accuracy: 0.9125 - val_auc: 0.8009\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1882 - accuracy: 0.9656 - auc: 0.9823 - val_loss: 0.1999 - val_accuracy: 0.9375 - val_auc: 0.8009\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1769 - accuracy: 0.9469 - auc: 0.9930 - val_loss: 0.1936 - val_accuracy: 0.9375 - val_auc: 0.8182\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1877 - accuracy: 0.9594 - auc: 0.9854 - val_loss: 0.1914 - val_accuracy: 0.9375 - val_auc: 0.8160\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9531 - auc: 0.9912 - val_loss: 0.1919 - val_accuracy: 0.9375 - val_auc: 0.8139\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1172 - accuracy: 0.9656 - auc: 0.9964 - val_loss: 0.1890 - val_accuracy: 0.9375 - val_auc: 0.8117\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9688 - auc: 0.9993 - val_loss: 0.1827 - val_accuracy: 0.9375 - val_auc: 0.8095\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1159 - accuracy: 0.9656 - auc: 0.9964 - val_loss: 0.1783 - val_accuracy: 0.9375 - val_auc: 0.8095\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1446 - accuracy: 0.9531 - auc: 0.9929 - val_loss: 0.1723 - val_accuracy: 0.9375 - val_auc: 0.8117\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1014 - accuracy: 0.9812 - auc: 0.9975 - val_loss: 0.1696 - val_accuracy: 0.9375 - val_auc: 0.8117\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9656 - auc: 0.9954 - val_loss: 0.1645 - val_accuracy: 0.9375 - val_auc: 0.8268\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.9812 - auc: 0.9989 - val_loss: 0.1581 - val_accuracy: 0.9375 - val_auc: 0.8636\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0828 - accuracy: 0.9719 - auc: 0.9989 - val_loss: 0.1529 - val_accuracy: 0.9375 - val_auc: 0.8680\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0879 - accuracy: 0.9875 - auc: 0.9989 - val_loss: 0.1482 - val_accuracy: 0.9375 - val_auc: 0.8680\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1297 - accuracy: 0.9844 - auc: 0.9955 - val_loss: 0.1465 - val_accuracy: 0.9375 - val_auc: 0.8723\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0835 - accuracy: 0.9688 - auc: 0.9968 - val_loss: 0.1464 - val_accuracy: 0.9375 - val_auc: 0.8658\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0897 - accuracy: 0.9781 - auc: 0.9996 - val_loss: 0.1452 - val_accuracy: 0.9375 - val_auc: 0.8528\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0533 - accuracy: 0.9812 - auc: 0.9979 - val_loss: 0.1447 - val_accuracy: 0.9500 - val_auc: 0.8615\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0605 - accuracy: 0.9906 - auc: 0.9986 - val_loss: 0.1422 - val_accuracy: 0.9625 - val_auc: 0.8571\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0624 - accuracy: 0.9812 - auc: 0.9991 - val_loss: 0.1401 - val_accuracy: 0.9625 - val_auc: 0.8550\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0441 - accuracy: 0.9844 - auc: 1.0000 - val_loss: 0.1389 - val_accuracy: 0.9625 - val_auc: 0.8593\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "测试集 DNN Accuracy for outcome_1: 0.9500\n",
      "测试集 DNN AUC for outcome_1: 0.6151\n",
      "\n",
      "--- 训练和评估 Outcome: outcome_2 使用DNN ---\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                3264      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5377 (21.00 KB)\n",
      "Trainable params: 5377 (21.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "类别权重 for outcome_2: {0: 0.5263157894736842, 1: 10.0}\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 30ms/step - loss: 0.7327 - accuracy: 0.6156 - auc: 0.5345 - val_loss: 0.5619 - val_accuracy: 0.7625 - val_auc: 0.6253\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6554 - accuracy: 0.7094 - auc: 0.6609 - val_loss: 0.5108 - val_accuracy: 0.8000 - val_auc: 0.6760\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5749 - accuracy: 0.7656 - auc: 0.7592 - val_loss: 0.4917 - val_accuracy: 0.8250 - val_auc: 0.7227\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5538 - accuracy: 0.7812 - auc: 0.7678 - val_loss: 0.4758 - val_accuracy: 0.8250 - val_auc: 0.7373\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.8156 - auc: 0.8532 - val_loss: 0.4581 - val_accuracy: 0.8375 - val_auc: 0.8000\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.4742 - accuracy: 0.7719 - auc: 0.8660 - val_loss: 0.4450 - val_accuracy: 0.8500 - val_auc: 0.8280\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5057 - accuracy: 0.8125 - auc: 0.8294 - val_loss: 0.4215 - val_accuracy: 0.8625 - val_auc: 0.8347\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4386 - accuracy: 0.8438 - auc: 0.8942 - val_loss: 0.4167 - val_accuracy: 0.8625 - val_auc: 0.8573\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3537 - accuracy: 0.8188 - auc: 0.9572 - val_loss: 0.3892 - val_accuracy: 0.8625 - val_auc: 0.8667\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3867 - accuracy: 0.8687 - auc: 0.9330 - val_loss: 0.3776 - val_accuracy: 0.8500 - val_auc: 0.8667\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3311 - accuracy: 0.8625 - auc: 0.9693 - val_loss: 0.3591 - val_accuracy: 0.8875 - val_auc: 0.8693\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3002 - accuracy: 0.9000 - auc: 0.9691 - val_loss: 0.3357 - val_accuracy: 0.8875 - val_auc: 0.8680\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3235 - accuracy: 0.9000 - auc: 0.9501 - val_loss: 0.3141 - val_accuracy: 0.8875 - val_auc: 0.8707\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.2521 - accuracy: 0.9125 - auc: 0.9819 - val_loss: 0.2983 - val_accuracy: 0.8875 - val_auc: 0.8720\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2989 - accuracy: 0.9125 - auc: 0.9623 - val_loss: 0.2916 - val_accuracy: 0.8875 - val_auc: 0.8733\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2872 - accuracy: 0.9062 - auc: 0.9681 - val_loss: 0.2865 - val_accuracy: 0.8875 - val_auc: 0.8733\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2596 - accuracy: 0.9187 - auc: 0.9678 - val_loss: 0.2807 - val_accuracy: 0.8875 - val_auc: 0.8787\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1706 - accuracy: 0.9187 - auc: 0.9948 - val_loss: 0.2667 - val_accuracy: 0.8875 - val_auc: 0.8773\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.9531 - auc: 0.9939 - val_loss: 0.2528 - val_accuracy: 0.9000 - val_auc: 0.8760\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1781 - accuracy: 0.9469 - auc: 0.9892 - val_loss: 0.2422 - val_accuracy: 0.9000 - val_auc: 0.8720\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1390 - accuracy: 0.9375 - auc: 0.9973 - val_loss: 0.2283 - val_accuracy: 0.9125 - val_auc: 0.8720\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1971 - accuracy: 0.9563 - auc: 0.9897 - val_loss: 0.2243 - val_accuracy: 0.9125 - val_auc: 0.8693\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1092 - accuracy: 0.9625 - auc: 0.9972 - val_loss: 0.2153 - val_accuracy: 0.9125 - val_auc: 0.8693\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1190 - accuracy: 0.9656 - auc: 0.9963 - val_loss: 0.2095 - val_accuracy: 0.9125 - val_auc: 0.8693\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 0.9625 - auc: 0.9938 - val_loss: 0.2074 - val_accuracy: 0.9125 - val_auc: 0.8627\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1956 - accuracy: 0.9500 - auc: 0.9803 - val_loss: 0.2095 - val_accuracy: 0.9125 - val_auc: 0.8600\n",
      "Epoch 27/50\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9375 - auc: 0.0000e+00Restoring model weights from the end of the best epoch: 17.\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1022 - accuracy: 0.9656 - auc: 0.9985 - val_loss: 0.2087 - val_accuracy: 0.9000 - val_auc: 0.8627\n",
      "Epoch 27: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "测试集 DNN Accuracy for outcome_2: 0.8800\n",
      "测试集 DNN AUC for outcome_2: 0.4547\n",
      "\n",
      "--- 训练和评估 Outcome: outcome_3 使用DNN ---\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 64)                3264      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5377 (21.00 KB)\n",
      "Trainable params: 5377 (21.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "类别权重 for outcome_3: {0: 0.5633802816901409, 1: 4.444444444444445}\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 36ms/step - loss: 0.8189 - accuracy: 0.5312 - auc: 0.4926 - val_loss: 0.7301 - val_accuracy: 0.5250 - val_auc: 0.2928\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.7794 - accuracy: 0.5500 - auc: 0.5609 - val_loss: 0.6995 - val_accuracy: 0.5750 - val_auc: 0.3412\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.7366 - accuracy: 0.5594 - auc: 0.5783 - val_loss: 0.6782 - val_accuracy: 0.5875 - val_auc: 0.3761\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6794 - accuracy: 0.5938 - auc: 0.6613 - val_loss: 0.6465 - val_accuracy: 0.6875 - val_auc: 0.4088\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6903 - accuracy: 0.6219 - auc: 0.6579 - val_loss: 0.6155 - val_accuracy: 0.7375 - val_auc: 0.4437\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5772 - accuracy: 0.7094 - auc: 0.8128 - val_loss: 0.5947 - val_accuracy: 0.7625 - val_auc: 0.4786\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6210 - accuracy: 0.6812 - auc: 0.7437 - val_loss: 0.5675 - val_accuracy: 0.7875 - val_auc: 0.4820\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5509 - accuracy: 0.7594 - auc: 0.8324 - val_loss: 0.5498 - val_accuracy: 0.8000 - val_auc: 0.5045\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.5235 - accuracy: 0.7656 - auc: 0.8575 - val_loss: 0.5329 - val_accuracy: 0.7750 - val_auc: 0.5259\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4823 - accuracy: 0.7812 - auc: 0.9022 - val_loss: 0.5115 - val_accuracy: 0.8000 - val_auc: 0.5394\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.4555 - accuracy: 0.8156 - auc: 0.8981 - val_loss: 0.4879 - val_accuracy: 0.8000 - val_auc: 0.5473\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4177 - accuracy: 0.8469 - auc: 0.9301 - val_loss: 0.4712 - val_accuracy: 0.8125 - val_auc: 0.5777\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4262 - accuracy: 0.8313 - auc: 0.9165 - val_loss: 0.4585 - val_accuracy: 0.8375 - val_auc: 0.5968\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3742 - accuracy: 0.8594 - auc: 0.9536 - val_loss: 0.4361 - val_accuracy: 0.8500 - val_auc: 0.5991\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3374 - accuracy: 0.8656 - auc: 0.9582 - val_loss: 0.4182 - val_accuracy: 0.8500 - val_auc: 0.6002\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3544 - accuracy: 0.8875 - auc: 0.9554 - val_loss: 0.4129 - val_accuracy: 0.8500 - val_auc: 0.6070\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3361 - accuracy: 0.8781 - auc: 0.9534 - val_loss: 0.4105 - val_accuracy: 0.8500 - val_auc: 0.6115\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2889 - accuracy: 0.8781 - auc: 0.9664 - val_loss: 0.3991 - val_accuracy: 0.8500 - val_auc: 0.6115\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2830 - accuracy: 0.9000 - auc: 0.9726 - val_loss: 0.3849 - val_accuracy: 0.8500 - val_auc: 0.6171\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2328 - accuracy: 0.9250 - auc: 0.9838 - val_loss: 0.3836 - val_accuracy: 0.8500 - val_auc: 0.6250\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2680 - accuracy: 0.9094 - auc: 0.9703 - val_loss: 0.3897 - val_accuracy: 0.8500 - val_auc: 0.6329\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.2496 - accuracy: 0.8875 - auc: 0.9729 - val_loss: 0.3960 - val_accuracy: 0.8500 - val_auc: 0.6374\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1989 - accuracy: 0.9250 - auc: 0.9870 - val_loss: 0.3830 - val_accuracy: 0.8500 - val_auc: 0.6408\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2006 - accuracy: 0.9406 - auc: 0.9849 - val_loss: 0.3804 - val_accuracy: 0.8500 - val_auc: 0.6509\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1726 - accuracy: 0.9469 - auc: 0.9926 - val_loss: 0.3838 - val_accuracy: 0.8500 - val_auc: 0.6622\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9531 - auc: 0.9933 - val_loss: 0.3777 - val_accuracy: 0.8500 - val_auc: 0.6509\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1449 - accuracy: 0.9469 - auc: 0.9940 - val_loss: 0.3789 - val_accuracy: 0.8500 - val_auc: 0.6655\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1537 - accuracy: 0.9375 - auc: 0.9920 - val_loss: 0.3754 - val_accuracy: 0.8500 - val_auc: 0.6655\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1417 - accuracy: 0.9688 - auc: 0.9958 - val_loss: 0.3844 - val_accuracy: 0.8500 - val_auc: 0.6813\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1455 - accuracy: 0.9563 - auc: 0.9927 - val_loss: 0.3940 - val_accuracy: 0.8500 - val_auc: 0.6914\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.1322 - accuracy: 0.9625 - auc: 0.9935 - val_loss: 0.3962 - val_accuracy: 0.8500 - val_auc: 0.6993\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0913 - accuracy: 0.9563 - auc: 0.9993 - val_loss: 0.3955 - val_accuracy: 0.8500 - val_auc: 0.6993\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1307 - accuracy: 0.9500 - auc: 0.9927 - val_loss: 0.3960 - val_accuracy: 0.8500 - val_auc: 0.6948\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1139 - accuracy: 0.9531 - auc: 0.9939 - val_loss: 0.3997 - val_accuracy: 0.8500 - val_auc: 0.6948\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1077 - accuracy: 0.9500 - auc: 0.9957 - val_loss: 0.4046 - val_accuracy: 0.8625 - val_auc: 0.7061\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0915 - accuracy: 0.9625 - auc: 0.9967 - val_loss: 0.4024 - val_accuracy: 0.8500 - val_auc: 0.7027\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0640 - accuracy: 0.9719 - auc: 0.9995 - val_loss: 0.4027 - val_accuracy: 0.8500 - val_auc: 0.7151\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0702 - accuracy: 0.9844 - auc: 0.9999 - val_loss: 0.4071 - val_accuracy: 0.8500 - val_auc: 0.7162\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0951 - accuracy: 0.9656 - auc: 0.9960 - val_loss: 0.4127 - val_accuracy: 0.8625 - val_auc: 0.7117\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9719 - auc: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.8625 - val_auc: 0.7162\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0628 - accuracy: 0.9781 - auc: 0.9990 - val_loss: 0.4307 - val_accuracy: 0.8625 - val_auc: 0.7185\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0689 - accuracy: 0.9781 - auc: 0.9983 - val_loss: 0.4399 - val_accuracy: 0.8625 - val_auc: 0.7162\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 0.9719 - auc: 0.9979 - val_loss: 0.4423 - val_accuracy: 0.8625 - val_auc: 0.7162\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9719 - auc: 0.9990 - val_loss: 0.4467 - val_accuracy: 0.8625 - val_auc: 0.7151\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0766 - accuracy: 0.9750 - auc: 0.9988 - val_loss: 0.4631 - val_accuracy: 0.8625 - val_auc: 0.7061\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.9750 - auc: 0.9993 - val_loss: 0.4737 - val_accuracy: 0.8625 - val_auc: 0.7095\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0730 - accuracy: 0.9688 - auc: 0.9971 - val_loss: 0.4761 - val_accuracy: 0.8500 - val_auc: 0.7185\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0448 - accuracy: 0.9844 - auc: 0.9996 - val_loss: 0.4760 - val_accuracy: 0.8625 - val_auc: 0.7106\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0657 - accuracy: 0.9781 - auc: 0.9980 - val_loss: 0.4790 - val_accuracy: 0.8500 - val_auc: 0.7128\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9812 - auc: 0.9994 - val_loss: 0.4831 - val_accuracy: 0.8750 - val_auc: 0.7241\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "测试集 DNN Accuracy for outcome_3: 0.9500\n",
      "测试集 DNN AUC for outcome_3: 0.9216\n",
      "\n",
      "\n",
      "--- DNN 单任务模型结果总结 ---\n",
      "\n",
      "Outcome: outcome_1\n",
      "  Accuracy: 0.9500\n",
      "  AUC: 0.6151\n",
      "  训练停止时的验证集AUC (best): 0.8723\n",
      "\n",
      "Outcome: outcome_2\n",
      "  Accuracy: 0.8800\n",
      "  AUC: 0.4547\n",
      "  训练停止时的验证集AUC (best): 0.8787\n",
      "\n",
      "Outcome: outcome_3\n",
      "  Accuracy: 0.9500\n",
      "  AUC: 0.9216\n",
      "  训练停止时的验证集AUC (best): 0.7241\n"
     ]
    }
   ],
   "source": [
    "# --- 新增：DNN 模型创建函数 ---\n",
    "def create_dnn_model(input_dim, hidden_units=[64, 32], dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    创建一个简单的DNN模型用于二分类。\n",
    "\n",
    "    参数:\n",
    "    input_dim (int): 输入特征的数量。\n",
    "    hidden_units (list of int): 每个隐藏层中的神经元数量。\n",
    "    dropout_rate (float): Dropout比率。\n",
    "\n",
    "    返回:\n",
    "    tensorflow.keras.models.Sequential: 编译好的Keras模型。\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units[0], input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if len(hidden_units) > 1:\n",
    "        for units in hidden_units[1:]:\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid')) # 二分类输出层\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "dnn_results = {}\n",
    "# DNN训练参数\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "# 定义提前停止回调\n",
    "early_stopping = EarlyStopping(monitor='val_auc', mode='max', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "\n",
    "for i in range(num_outcomes):\n",
    "    outcome_col_name = f'outcome_{i+1}'\n",
    "    print(f\"\\n--- 训练和评估 Outcome: {outcome_col_name} 使用DNN ---\")\n",
    "\n",
    "    y_train_outcome = Y_train[outcome_col_name].values # Keras 通常期望 numpy array\n",
    "    y_test_outcome = Y_test[outcome_col_name].values\n",
    "\n",
    "    if len(np.unique(y_train_outcome)) < 2:\n",
    "        print(f\"Outcome {outcome_col_name} 在训练集中只有一个类别，跳过此outcome。\")\n",
    "        dnn_results[outcome_col_name] = {'accuracy': np.nan, 'auc': np.nan, 'history': None, 'weights': None}\n",
    "        continue\n",
    "\n",
    "    # --- 创建和训练DNN模型 ---\n",
    "    dnn_model = create_dnn_model(input_dim=X_train.shape[1])\n",
    "    print(dnn_model.summary())\n",
    "\n",
    "    # 计算类别权重以处理不平衡数据\n",
    "    neg_count = np.sum(y_train_outcome == 0)\n",
    "    pos_count = np.sum(y_train_outcome == 1)\n",
    "    total = neg_count + pos_count\n",
    "\n",
    "    # 权重计算公式: weight_for_0 = (1 / neg_count) * (total / 2.0)\n",
    "    #              weight_for_1 = (1 / pos_count) * (total / 2.0)\n",
    "    # Keras期望一个字典: {class_0_label: weight_0, class_1_label: weight_1}\n",
    "    class_weights = {\n",
    "        0: (1 / neg_count) * (total / 2.0) if neg_count > 0 else 1,\n",
    "        1: (1 / pos_count) * (total / 2.0) if pos_count > 0 else 1\n",
    "    }\n",
    "    print(f\"类别权重 for {outcome_col_name}: {class_weights}\")\n",
    "\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        X_train,\n",
    "        y_train_outcome,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2, # 从训练集中分出一部分作为验证集\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights, # 应用类别权重\n",
    "        verbose=1 # 设置为0以减少训练过程中的输出，设置为1或2以查看进度\n",
    "    )\n",
    "\n",
    "    # --- 获取模型权重 (可选，DNN的权重解释性不如线性模型) ---\n",
    "    # 这里可以获取第一层权重作为示例\n",
    "    first_layer_weights = dnn_model.layers[0].get_weights()[0] # [0]是权重矩阵，[1]是偏置\n",
    "\n",
    "    # --- 在测试集上进行评估 ---\n",
    "    # predict返回的是(0,1)之间的概率值\n",
    "    y_pred_proba_dnn = dnn_model.predict(X_test)\n",
    "    # 将概率转换为二元预测 (阈值0.5)\n",
    "    y_pred_binary_dnn = (y_pred_proba_dnn > 0.5).astype(int).flatten() # flatten() 以匹配y_test_outcome的形状\n",
    "\n",
    "    # --- 计算并记录 Accuracy 和 AUC ---\n",
    "    accuracy_dnn = calculate_accuracy(y_test_outcome, y_pred_binary_dnn)\n",
    "    auc_dnn = calculate_auc(y_test_outcome, y_pred_proba_dnn) # y_pred_proba_dnn已经是正类的概率\n",
    "\n",
    "    dnn_results[outcome_col_name] = {\n",
    "        'accuracy': accuracy_dnn,\n",
    "        'auc': auc_dnn,\n",
    "        'history': history.history, # 存储训练历史，方便后续绘图\n",
    "        'weights_layer0': first_layer_weights\n",
    "    }\n",
    "\n",
    "    print(f\"测试集 DNN Accuracy for {outcome_col_name}: {accuracy_dnn:.4f}\")\n",
    "    print(f\"测试集 DNN AUC for {outcome_col_name}: {auc_dnn:.4f}\")\n",
    "\n",
    "    # 可选：绘制训练过程中的损失和AUC曲线\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(history.history['loss'], label='Train Loss')\n",
    "    # plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    # plt.title(f'Loss for {outcome_col_name}')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(history.history['auc'], label='Train AUC')\n",
    "    # plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    # plt.title(f'AUC for {outcome_col_name}')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('AUC')\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- DNN 单任务模型结果总结 ---\")\n",
    "for outcome, metrics in dnn_results.items():\n",
    "    if metrics['history'] is not None: # 仅打印成功训练的outcome\n",
    "        print(f\"\\nOutcome: {outcome}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  AUC: {metrics['auc']:.4f}\")\n",
    "        print(f\"  训练停止时的验证集AUC (best): {max(metrics['history'].get('val_auc', [np.nan])):.4f}\")\n",
    "        # print(f\"  第一层权重形状: {metrics['weights_layer0'].shape if metrics['weights_layer0'] is not None else 'N/A'}\")\n",
    "    else:\n",
    "        print(f\"\\nOutcome: {outcome} - 训练跳过\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100a1f3-2be4-46f6-a41f-43a8ae2af87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1309cf00-32d9-4c2b-9824-3e62b7a82a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始使用MTL DNN模型训练与评估 ---\n",
      "Model: \"mtl_dnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_features (InputLayer  [(None, 50)]                 0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 128)                  6528      ['input_features[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 128)                  0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 64)                   8256      ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 64)                   0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " task_1_specific_dense_32 (  (None, 32)                   2080      ['dropout_7[0][0]']           \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " task_2_specific_dense_32 (  (None, 32)                   2080      ['dropout_7[0][0]']           \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " task_3_specific_dense_32 (  (None, 32)                   2080      ['dropout_7[0][0]']           \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " task_1_specific_dropout (D  (None, 32)                   0         ['task_1_specific_dense_32[0][\n",
      " ropout)                                                            0]']                          \n",
      "                                                                                                  \n",
      " task_2_specific_dropout (D  (None, 32)                   0         ['task_2_specific_dense_32[0][\n",
      " ropout)                                                            0]']                          \n",
      "                                                                                                  \n",
      " task_3_specific_dropout (D  (None, 32)                   0         ['task_3_specific_dense_32[0][\n",
      " ropout)                                                            0]']                          \n",
      "                                                                                                  \n",
      " outcome_1 (Dense)           (None, 1)                    33        ['task_1_specific_dropout[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " outcome_2 (Dense)           (None, 1)                    33        ['task_2_specific_dropout[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " outcome_3 (Dense)           (None, 1)                    33        ['task_3_specific_dropout[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21123 (82.51 KB)\n",
      "Trainable params: 21123 (82.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "10/10 [==============================] - 3s 74ms/step - loss: 1.7137 - outcome_1_loss: 0.6042 - outcome_2_loss: 0.6242 - outcome_3_loss: 0.4853 - outcome_1_accuracy: 0.6469 - outcome_1_auc_1: 0.4923 - outcome_2_accuracy: 0.6250 - outcome_2_auc_2: 0.5331 - outcome_3_accuracy: 0.8031 - outcome_3_auc_3: 0.5339 - val_loss: 1.1404 - val_outcome_1_loss: 0.3622 - val_outcome_2_loss: 0.4261 - val_outcome_3_loss: 0.3522 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.4113 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.4600 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6396\n",
      "Epoch 2/60\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.1231 - outcome_1_loss: 0.3350 - outcome_2_loss: 0.3871 - outcome_3_loss: 0.4010 - outcome_1_accuracy: 0.8938 - outcome_1_auc_1: 0.4745 - outcome_2_accuracy: 0.8750 - outcome_2_auc_2: 0.5649 - outcome_3_accuracy: 0.8687 - outcome_3_auc_3: 0.5779 - val_loss: 0.7940 - val_outcome_1_loss: 0.2067 - val_outcome_2_loss: 0.2939 - val_outcome_3_loss: 0.2935 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.4502 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.4587 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6543\n",
      "Epoch 3/60\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8986 - outcome_1_loss: 0.1981 - outcome_2_loss: 0.3035 - outcome_3_loss: 0.3969 - outcome_1_accuracy: 0.9563 - outcome_1_auc_1: 0.5732 - outcome_2_accuracy: 0.9250 - outcome_2_auc_2: 0.4901 - outcome_3_accuracy: 0.8781 - outcome_3_auc_3: 0.6025 - val_loss: 0.6920 - val_outcome_1_loss: 0.1681 - val_outcome_2_loss: 0.2495 - val_outcome_3_loss: 0.2744 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.4762 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.4413 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6599\n",
      "Epoch 4/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.7656 - outcome_1_loss: 0.1847 - outcome_2_loss: 0.2106 - outcome_3_loss: 0.3703 - outcome_1_accuracy: 0.9688 - outcome_1_auc_1: 0.4632 - outcome_2_accuracy: 0.9563 - outcome_2_auc_2: 0.6762 - outcome_3_accuracy: 0.8781 - outcome_3_auc_3: 0.6674 - val_loss: 0.6785 - val_outcome_1_loss: 0.1651 - val_outcome_2_loss: 0.2446 - val_outcome_3_loss: 0.2688 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.5281 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.4787 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6689\n",
      "Epoch 5/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.7680 - outcome_1_loss: 0.1500 - outcome_2_loss: 0.2278 - outcome_3_loss: 0.3903 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.6443 - outcome_2_accuracy: 0.9438 - outcome_2_auc_2: 0.5444 - outcome_3_accuracy: 0.8813 - outcome_3_auc_3: 0.6169 - val_loss: 0.6851 - val_outcome_1_loss: 0.1643 - val_outcome_2_loss: 0.2457 - val_outcome_3_loss: 0.2751 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.5887 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.5200 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6599\n",
      "Epoch 6/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.7576 - outcome_1_loss: 0.1672 - outcome_2_loss: 0.2364 - outcome_3_loss: 0.3541 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.5754 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.4317 - outcome_3_accuracy: 0.8750 - outcome_3_auc_3: 0.6924 - val_loss: 0.6893 - val_outcome_1_loss: 0.1620 - val_outcome_2_loss: 0.2454 - val_outcome_3_loss: 0.2819 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6407 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.5587 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6678\n",
      "Epoch 7/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6934 - outcome_1_loss: 0.1530 - outcome_2_loss: 0.1925 - outcome_3_loss: 0.3479 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.6642 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.7320 - outcome_3_accuracy: 0.8750 - outcome_3_auc_3: 0.7045 - val_loss: 0.6890 - val_outcome_1_loss: 0.1574 - val_outcome_2_loss: 0.2404 - val_outcome_3_loss: 0.2912 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6515 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6013 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6700\n",
      "Epoch 8/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.6997 - outcome_1_loss: 0.1420 - outcome_2_loss: 0.1968 - outcome_3_loss: 0.3609 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.6679 - outcome_2_accuracy: 0.9469 - outcome_2_auc_2: 0.6635 - outcome_3_accuracy: 0.8813 - outcome_3_auc_3: 0.6671 - val_loss: 0.6802 - val_outcome_1_loss: 0.1546 - val_outcome_2_loss: 0.2373 - val_outcome_3_loss: 0.2882 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6710 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6093 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6678\n",
      "Epoch 9/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6474 - outcome_1_loss: 0.1466 - outcome_2_loss: 0.1736 - outcome_3_loss: 0.3271 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.6661 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.7349 - outcome_3_accuracy: 0.8719 - outcome_3_auc_3: 0.7472 - val_loss: 0.6648 - val_outcome_1_loss: 0.1520 - val_outcome_2_loss: 0.2357 - val_outcome_3_loss: 0.2771 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7338 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6213 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6644\n",
      "Epoch 10/60\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7069 - outcome_1_loss: 0.1542 - outcome_2_loss: 0.1911 - outcome_3_loss: 0.3616 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.6397 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.6872 - outcome_3_accuracy: 0.8750 - outcome_3_auc_3: 0.6964 - val_loss: 0.6584 - val_outcome_1_loss: 0.1520 - val_outcome_2_loss: 0.2374 - val_outcome_3_loss: 0.2690 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7619 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6667 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6689\n",
      "Epoch 11/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5939 - outcome_1_loss: 0.1221 - outcome_2_loss: 0.1962 - outcome_3_loss: 0.2756 - outcome_1_accuracy: 0.9656 - outcome_1_auc_1: 0.7865 - outcome_2_accuracy: 0.9500 - outcome_2_auc_2: 0.6833 - outcome_3_accuracy: 0.8813 - outcome_3_auc_3: 0.8585 - val_loss: 0.6496 - val_outcome_1_loss: 0.1483 - val_outcome_2_loss: 0.2332 - val_outcome_3_loss: 0.2681 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7814 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6653 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6847\n",
      "Epoch 12/60\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6290 - outcome_1_loss: 0.1313 - outcome_2_loss: 0.1883 - outcome_3_loss: 0.3094 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.6869 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.7113 - outcome_3_accuracy: 0.8750 - outcome_3_auc_3: 0.7931 - val_loss: 0.6421 - val_outcome_1_loss: 0.1438 - val_outcome_2_loss: 0.2281 - val_outcome_3_loss: 0.2703 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.8268 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6893 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6802\n",
      "Epoch 13/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.5753 - outcome_1_loss: 0.1176 - outcome_2_loss: 0.1618 - outcome_3_loss: 0.2959 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.8035 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.7890 - outcome_3_accuracy: 0.8781 - outcome_3_auc_3: 0.8149 - val_loss: 0.6381 - val_outcome_1_loss: 0.1426 - val_outcome_2_loss: 0.2274 - val_outcome_3_loss: 0.2681 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.8377 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6947 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6734\n",
      "Epoch 14/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5436 - outcome_1_loss: 0.1116 - outcome_2_loss: 0.1545 - outcome_3_loss: 0.2775 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.7930 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.8226 - outcome_3_accuracy: 0.8813 - outcome_3_auc_3: 0.8517 - val_loss: 0.6379 - val_outcome_1_loss: 0.1444 - val_outcome_2_loss: 0.2304 - val_outcome_3_loss: 0.2632 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.8463 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7013 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6723\n",
      "Epoch 15/60\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.4985 - outcome_1_loss: 0.0906 - outcome_2_loss: 0.1448 - outcome_3_loss: 0.2630 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9264 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.8541 - outcome_3_accuracy: 0.8781 - outcome_3_auc_3: 0.8771 - val_loss: 0.6406 - val_outcome_1_loss: 0.1458 - val_outcome_2_loss: 0.2324 - val_outcome_3_loss: 0.2625 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7879 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7267 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6689\n",
      "Epoch 16/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5179 - outcome_1_loss: 0.1082 - outcome_2_loss: 0.1403 - outcome_3_loss: 0.2695 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.8510 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.8792 - outcome_3_accuracy: 0.8844 - outcome_3_auc_3: 0.8645 - val_loss: 0.6452 - val_outcome_1_loss: 0.1470 - val_outcome_2_loss: 0.2349 - val_outcome_3_loss: 0.2632 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.8182 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.6907 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6712\n",
      "Epoch 17/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.5231 - outcome_1_loss: 0.1045 - outcome_2_loss: 0.1550 - outcome_3_loss: 0.2637 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.8062 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.8425 - outcome_3_accuracy: 0.8938 - outcome_3_auc_3: 0.8674 - val_loss: 0.6512 - val_outcome_1_loss: 0.1500 - val_outcome_2_loss: 0.2372 - val_outcome_3_loss: 0.2641 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7879 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7093 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6734\n",
      "Epoch 18/60\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4775 - outcome_1_loss: 0.1019 - outcome_2_loss: 0.1205 - outcome_3_loss: 0.2551 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.8403 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.9314 - outcome_3_accuracy: 0.8844 - outcome_3_auc_3: 0.8876 - val_loss: 0.6534 - val_outcome_1_loss: 0.1521 - val_outcome_2_loss: 0.2363 - val_outcome_3_loss: 0.2649 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7835 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7080 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6824\n",
      "Epoch 19/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4558 - outcome_1_loss: 0.0891 - outcome_2_loss: 0.1296 - outcome_3_loss: 0.2371 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9089 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.9110 - outcome_3_accuracy: 0.8687 - outcome_3_auc_3: 0.9193 - val_loss: 0.6671 - val_outcome_1_loss: 0.1569 - val_outcome_2_loss: 0.2418 - val_outcome_3_loss: 0.2684 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7597 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7147 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6712\n",
      "Epoch 20/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.4812 - outcome_1_loss: 0.0899 - outcome_2_loss: 0.1400 - outcome_3_loss: 0.2512 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9032 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.8760 - outcome_3_accuracy: 0.8813 - outcome_3_auc_3: 0.8866 - val_loss: 0.6736 - val_outcome_1_loss: 0.1584 - val_outcome_2_loss: 0.2437 - val_outcome_3_loss: 0.2715 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7338 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7347 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6723\n",
      "Epoch 21/60\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4116 - outcome_1_loss: 0.0794 - outcome_2_loss: 0.1178 - outcome_3_loss: 0.2144 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9132 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.9283 - outcome_3_accuracy: 0.8969 - outcome_3_auc_3: 0.9331 - val_loss: 0.6715 - val_outcome_1_loss: 0.1572 - val_outcome_2_loss: 0.2399 - val_outcome_3_loss: 0.2745 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7662 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7320 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6745\n",
      "Epoch 22/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.4388 - outcome_1_loss: 0.0908 - outcome_2_loss: 0.1341 - outcome_3_loss: 0.2139 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9103 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.8930 - outcome_3_accuracy: 0.8906 - outcome_3_auc_3: 0.9382 - val_loss: 0.6730 - val_outcome_1_loss: 0.1564 - val_outcome_2_loss: 0.2392 - val_outcome_3_loss: 0.2774 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7641 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7427 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6734\n",
      "Epoch 23/60\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3828 - outcome_1_loss: 0.0762 - outcome_2_loss: 0.1068 - outcome_3_loss: 0.1998 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9652 - outcome_2_accuracy: 0.9531 - outcome_2_auc_2: 0.9637 - outcome_3_accuracy: 0.9094 - outcome_3_auc_3: 0.9428 - val_loss: 0.6887 - val_outcome_1_loss: 0.1604 - val_outcome_2_loss: 0.2421 - val_outcome_3_loss: 0.2862 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7511 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7707 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6441\n",
      "Epoch 24/60\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.3552 - outcome_1_loss: 0.0788 - outcome_2_loss: 0.1092 - outcome_3_loss: 0.1671 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9323 - outcome_2_accuracy: 0.9563 - outcome_2_auc_2: 0.9421 - outcome_3_accuracy: 0.9438 - outcome_3_auc_3: 0.9649 - val_loss: 0.7114 - val_outcome_1_loss: 0.1653 - val_outcome_2_loss: 0.2464 - val_outcome_3_loss: 0.2997 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7727 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7427 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6520\n",
      "Epoch 25/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3716 - outcome_1_loss: 0.0852 - outcome_2_loss: 0.1102 - outcome_3_loss: 0.1762 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9148 - outcome_2_accuracy: 0.9594 - outcome_2_auc_2: 0.9200 - outcome_3_accuracy: 0.9219 - outcome_3_auc_3: 0.9602 - val_loss: 0.7390 - val_outcome_1_loss: 0.1692 - val_outcome_2_loss: 0.2536 - val_outcome_3_loss: 0.3162 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6818 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7520 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6385\n",
      "Epoch 26/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3494 - outcome_1_loss: 0.0666 - outcome_2_loss: 0.1017 - outcome_3_loss: 0.1812 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9739 - outcome_2_accuracy: 0.9563 - outcome_2_auc_2: 0.9593 - outcome_3_accuracy: 0.9031 - outcome_3_auc_3: 0.9525 - val_loss: 0.7511 - val_outcome_1_loss: 0.1706 - val_outcome_2_loss: 0.2537 - val_outcome_3_loss: 0.3268 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6926 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7573 - val_outcome_3_accuracy: 0.9250 - val_outcome_3_auc_3: 0.6610\n",
      "Epoch 27/60\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3372 - outcome_1_loss: 0.0715 - outcome_2_loss: 0.1132 - outcome_3_loss: 0.1526 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9098 - outcome_2_accuracy: 0.9563 - outcome_2_auc_2: 0.9407 - outcome_3_accuracy: 0.9438 - outcome_3_auc_3: 0.9695 - val_loss: 0.7587 - val_outcome_1_loss: 0.1704 - val_outcome_2_loss: 0.2510 - val_outcome_3_loss: 0.3374 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6948 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7600 - val_outcome_3_accuracy: 0.8875 - val_outcome_3_auc_3: 0.6667\n",
      "Epoch 28/60\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2900 - outcome_1_loss: 0.0704 - outcome_2_loss: 0.0916 - outcome_3_loss: 0.1280 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9475 - outcome_2_accuracy: 0.9594 - outcome_2_auc_2: 0.9744 - outcome_3_accuracy: 0.9406 - outcome_3_auc_3: 0.9835 - val_loss: 0.7705 - val_outcome_1_loss: 0.1712 - val_outcome_2_loss: 0.2530 - val_outcome_3_loss: 0.3463 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.6970 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7640 - val_outcome_3_accuracy: 0.8875 - val_outcome_3_auc_3: 0.6655\n",
      "Epoch 29/60\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1088 - outcome_1_loss: 0.0147 - outcome_2_loss: 0.0532 - outcome_3_loss: 0.0409 - outcome_1_accuracy: 1.0000 - outcome_1_auc_1: 0.0000e+00 - outcome_2_accuracy: 0.9688 - outcome_2_auc_2: 1.0000 - outcome_3_accuracy: 1.0000 - outcome_3_auc_3: 1.0000Restoring model weights from the end of the best epoch: 14.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2982 - outcome_1_loss: 0.0573 - outcome_2_loss: 0.0946 - outcome_3_loss: 0.1463 - outcome_1_accuracy: 0.9719 - outcome_1_auc_1: 0.9839 - outcome_2_accuracy: 0.9500 - outcome_2_auc_2: 0.9658 - outcome_3_accuracy: 0.9500 - outcome_3_auc_3: 0.9594 - val_loss: 0.7876 - val_outcome_1_loss: 0.1741 - val_outcome_2_loss: 0.2555 - val_outcome_3_loss: 0.3580 - val_outcome_1_accuracy: 0.9625 - val_outcome_1_auc_1: 0.7186 - val_outcome_2_accuracy: 0.9375 - val_outcome_2_auc_2: 0.7187 - val_outcome_3_accuracy: 0.8875 - val_outcome_3_auc_3: 0.6340\n",
      "Epoch 29: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "\n",
      "--- MTL DNN模型在测试集上的性能 ---\n",
      "Outcome: outcome_1\n",
      "  Accuracy: 0.9700\n",
      "  AUC: 0.6701\n",
      "Outcome: outcome_2\n",
      "  Accuracy: 0.9500\n",
      "  AUC: 0.4379\n",
      "Outcome: outcome_3\n",
      "  Accuracy: 0.8500\n",
      "  AUC: 0.7953\n"
     ]
    }
   ],
   "source": [
    "# --- 深度学习库导入 ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model # Functional API 用于多输出模型\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model # 用于可视化模型结构，可选\n",
    "\n",
    "# --- 新增：MTL DNN 模型创建函数 ---\n",
    "def create_mtl_dnn_model(input_dim, num_outcomes, shared_hidden_units=[64, 32], task_specific_hidden_units=[16], dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    创建一个多任务学习的DNN模型。\n",
    "\n",
    "    参数:\n",
    "    input_dim (int): 输入特征的数量。\n",
    "    num_outcomes (int): 任务（结果）的数量。\n",
    "    shared_hidden_units (list of int): 共享隐藏层中的神经元数量。\n",
    "    task_specific_hidden_units (list of int): 每个任务特定分支的隐藏层神经元数量（可选，如果为空则直接连接到输出）。\n",
    "    dropout_rate (float): Dropout比率。\n",
    "\n",
    "    返回:\n",
    "    tensorflow.keras.models.Model: 编译好的Keras MTL模型。\n",
    "    \"\"\"\n",
    "    # 输入层\n",
    "    input_layer = Input(shape=(input_dim,), name='input_features')\n",
    "\n",
    "    # 共享层\n",
    "    shared_layer = input_layer\n",
    "    for units in shared_hidden_units:\n",
    "        shared_layer = Dense(units, activation='relu')(shared_layer)\n",
    "        shared_layer = Dropout(dropout_rate)(shared_layer)\n",
    "\n",
    "    # 任务特定的输出层\n",
    "    output_layers = []\n",
    "    for i in range(num_outcomes):\n",
    "        task_layer = shared_layer # 从共享层的输出开始\n",
    "        # 可以为每个任务添加额外的特定隐藏层\n",
    "        for units in task_specific_hidden_units:\n",
    "            task_layer = Dense(units, activation='relu', name=f'task_{i+1}_specific_dense_{units}')(task_layer)\n",
    "            task_layer = Dropout(dropout_rate, name=f'task_{i+1}_specific_dropout')(task_layer)\n",
    "        output_layer = Dense(1, activation='sigmoid', name=f'outcome_{i+1}')(task_layer)\n",
    "        output_layers.append(output_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layers, name='mtl_dnn_model')\n",
    "\n",
    "    # 编译模型\n",
    "    # 每个输出层需要一个损失函数和对应的指标\n",
    "    # 如果所有任务都是二分类，损失函数都是 'binary_crossentropy'\n",
    "    losses = {f'outcome_{i+1}': 'binary_crossentropy' for i in range(num_outcomes)}\n",
    "    metrics = {f'outcome_{i+1}': ['accuracy', tf.keras.metrics.AUC(name=f'auc_{i+1}')] for i in range(num_outcomes)}\n",
    "    # 可以为不同的任务设置不同的损失权重\n",
    "    loss_weights = {f'outcome_{i+1}': 1.0 for i in range(num_outcomes)} # 默认权重相同\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=losses,\n",
    "                  metrics=metrics,\n",
    "                  loss_weights=loss_weights)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Keras MTL 模型期望Y是列表或字典形式，每个元素对应一个输出\n",
    "Y_train_list = [Y_train[f'outcome_{i+1}'].values for i in range(num_outcomes)]\n",
    "Y_test_list = [Y_test[f'outcome_{i+1}'].values for i in range(num_outcomes)]\n",
    "# 或者使用字典形式，与模型编译时的loss和metrics的key对应\n",
    "Y_train_dict = {f'outcome_{i+1}': Y_train[f'outcome_{i+1}'].values for i in range(num_outcomes)}\n",
    "Y_test_dict = {f'outcome_{i+1}': Y_test[f'outcome_{i+1}'].values for i in range(num_outcomes)}\n",
    "\n",
    "\n",
    "print(\"\\n--- 开始使用MTL DNN模型训练与评估 ---\")\n",
    "\n",
    "mtl_dnn_results = {}\n",
    "epochs_mtl = 60 # MTL可能需要更多epochs来学习共享表示和特定任务\n",
    "batch_size_mtl = 32\n",
    "# 定义提前停止回调，监控整体验证损失或特定任务的验证AUC\n",
    "# 这里监控 val_loss (整体加权损失)\n",
    "early_stopping_mtl = EarlyStopping(monitor='val_loss', mode='min', patience=15, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# --- 创建和训练MTL DNN模型 ---\n",
    "mtl_model = create_mtl_dnn_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_outcomes=num_outcomes,\n",
    "    shared_hidden_units=[128, 64], # 可以尝试不同的共享层配置\n",
    "    task_specific_hidden_units=[32], # 每个任务分支再加一个32神经元的隐藏层\n",
    "    dropout_rate=0.4\n",
    ")\n",
    "print(mtl_model.summary())\n",
    "# plot_model(mtl_model, to_file='mtl_dnn_model.png', show_shapes=True, show_layer_names=True) # 可选：保存模型结构图\n",
    "\n",
    "history_mtl = mtl_model.fit(\n",
    "    X_train,\n",
    "    Y_train_dict,\n",
    "    epochs=epochs_mtl,\n",
    "    batch_size=batch_size_mtl,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping_mtl],\n",
    "    class_weight=None, # 暂时移除 class_weight\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_proba_mtl_list = mtl_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- MTL DNN模型在测试集上的性能 ---\")\n",
    "for i in range(num_outcomes):\n",
    "    outcome_col_name = f'outcome_{i+1}'\n",
    "    y_test_outcome_single = Y_test_dict[outcome_col_name]\n",
    "    y_pred_proba_single = y_pred_proba_mtl_list[i]\n",
    "    y_pred_binary_single = (y_pred_proba_single > 0.5).astype(int).flatten()\n",
    "\n",
    "    accuracy_mtl = calculate_accuracy(y_test_outcome_single, y_pred_binary_single)\n",
    "    auc_mtl = calculate_auc(y_test_outcome_single, y_pred_proba_single)\n",
    "\n",
    "    mtl_dnn_results[outcome_col_name] = {\n",
    "        'accuracy': accuracy_mtl,\n",
    "        'auc': auc_mtl\n",
    "    }\n",
    "    print(f\"Outcome: {outcome_col_name}\")\n",
    "    print(f\"  Accuracy: {accuracy_mtl:.4f}\")\n",
    "    print(f\"  AUC: {auc_mtl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1e5d9-6bca-46aa-a5e2-955859826aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e38a257-0ce8-4b71-b623-8e0a2609b6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---Accuracy (AUC)---\n",
      "          Logistic Regression Lasso (L1 LogReg) DNN (Single Task)          MTL DNN\n",
      "Outcome                                                                           \n",
      "outcome_1     0.9100 (0.6186)   0.7800 (0.6151)   0.9500 (0.6151)  0.9700 (0.6701)\n",
      "outcome_2     0.8900 (0.6000)   0.8700 (0.6337)   0.8800 (0.4547)  0.9500 (0.4379)\n",
      "outcome_3     0.9100 (0.9357)   0.9100 (0.9851)   0.9500 (0.9216)  0.8500 (0.7953)\n"
     ]
    }
   ],
   "source": [
    "def display_results_table(baseline_res, lasso_res, dnn_res, mtl_dnn_res, total_features):\n",
    "    \"\"\"\n",
    "    将所有模型的结果汇总到一个表格中并打印。\n",
    "\n",
    "    参数:\n",
    "    baseline_res (dict): 逻辑回归（L2）的结果字典。\n",
    "    lasso_res (dict): Lasso逻辑回归（L1）的结果字典。\n",
    "    dnn_res (dict): 单任务DNN的结果字典。\n",
    "    mtl_dnn_res (dict): 多任务DNN的结果字典。\n",
    "    total_features (int): 特征总数，用于显示Lasso的稀疏性。\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\n---Accuracy (AUC)---\")\n",
    "\n",
    "    summary_data = []\n",
    "    outcome_names = [f'outcome_{i+1}' for i in range(num_outcomes)]\n",
    "\n",
    "    for outcome_name in outcome_names:\n",
    "        row = {'Outcome': outcome_name}\n",
    "\n",
    "        # Logistic Regression (Baseline)\n",
    "        if outcome_name in baseline_results and baseline_results[outcome_name].get('accuracy') is not None:\n",
    "            acc_lr = baseline_results[outcome_name]['accuracy']\n",
    "            auc_lr = baseline_results[outcome_name]['auc']\n",
    "            row['Logistic Regression'] = f\"{acc_lr:.4f} ({auc_lr:.4f})\"\n",
    "        else:\n",
    "            row['Logistic Regression'] = \"N/A (skipped)\"\n",
    "\n",
    "        # Lasso (L1 LogReg)\n",
    "        if outcome_name in lasso_results and lasso_results[outcome_name].get('accuracy') is not None:\n",
    "            acc_lasso = lasso_results[outcome_name]['accuracy']\n",
    "            auc_lasso = lasso_results[outcome_name]['auc']\n",
    "            row['Lasso (L1 LogReg)'] = f\"{acc_lasso:.4f} ({auc_lasso:.4f})\"\n",
    "        else:\n",
    "            row['Lasso (L1 LogReg)'] = \"N/A (skipped)\"\n",
    "\n",
    "        # DNN (Single Task)\n",
    "        if outcome_name in dnn_results and dnn_results[outcome_name].get('accuracy') is not None and not np.isnan(dnn_results[outcome_name]['accuracy']):\n",
    "            acc_dnn_stl = dnn_results[outcome_name]['accuracy']\n",
    "            auc_dnn_stl = dnn_results[outcome_name]['auc']\n",
    "            row['DNN (Single Task)'] = f\"{acc_dnn_stl:.4f} ({auc_dnn_stl:.4f})\"\n",
    "        else:\n",
    "            row['DNN (Single Task)'] = \"N/A (skipped)\"\n",
    "            if outcome_name in dnn_results and dnn_results[outcome_name].get('accuracy') is not None and np.isnan(dnn_results[outcome_name]['accuracy']):\n",
    "                 row['DNN (Single Task)'] = \"N/A (NaN result)\"\n",
    "\n",
    "\n",
    "        # MTL DNN\n",
    "        if outcome_name in mtl_dnn_results and mtl_dnn_results[outcome_name].get('accuracy') is not None:\n",
    "            acc_dnn_mtl = mtl_dnn_results[outcome_name]['accuracy']\n",
    "            auc_dnn_mtl = mtl_dnn_results[outcome_name]['auc']\n",
    "            row['MTL DNN'] = f\"{acc_dnn_mtl:.4f} ({auc_dnn_mtl:.4f})\"\n",
    "        else:\n",
    "            # MTL DNN 的结果是针对所有任务的，如果某个任务在训练时就有问题（例如类别不平衡到无法训练特定头部）\n",
    "            # 或者如果在评估时没有正确填充mtl_dnn_results，这里也会显示N/A\n",
    "            row['MTL DNN'] = \"N/A\"\n",
    "\n",
    "\n",
    "        summary_data.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.set_index('Outcome', inplace=True)\n",
    "\n",
    "    print(summary_df.to_string()) # 使用 to_string() 保证表格完整打印\n",
    "\n",
    "display_results_table(baseline_results, lasso_results, dnn_results, mtl_dnn_results, n_total_cols_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
